{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import os\n",
    "\n",
    "#setup tweepy Oauth\n",
    "auth = tweepy.OAuthHandler(\n",
    "    'YMaRCwObPMfcEvcqDSMaEza4x', \n",
    "    'qmawQuA8p6Rxvp9rrLREUkaLVRLRWpUdOE9GaI2BLpxntq8iat'\n",
    ")\n",
    "auth.set_access_token(\n",
    "    '961387368463781888-6NYJVeWtVhbXVU12qvRno4kuq43492N',\n",
    "    '5DsU7UfhghntczyLNsijssrfZVpcsOe8Rbo24Ighjnzwo'\n",
    ")\n",
    "api=tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvs_to_txt(inputs, output, column, cst=False):\n",
    "    out_file = open(output, \"w\")\n",
    "    for f in files:\n",
    "        if not cst:\n",
    "            df_tweets=pd.read_csv(f, error_bad_lines=False)\n",
    "        else:\n",
    "            df_tweets=pd.read_table(f, sep='\\t')\n",
    "        for text in df_tweets[column]:\n",
    "            out_file.write(str(text).replace('\\r', '').replace('\\n',' ')+\"\\n\")\n",
    "    out_file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrisisLexT6\n",
    "out = \"tweets_CrisisLexT6.txt\"\n",
    "files = []\n",
    "files.append(\"tweets/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\")\n",
    "files.append(\"tweets/CrisisLexT6/2013_Alberta_Floods/2013_Alberta_Floods-ontopic_offtopic.csv\")\n",
    "files.append(\"tweets/CrisisLexT6/2013_Boston_Bombings/2013_Boston_Bombings-ontopic_offtopic.csv\")\n",
    "files.append(\"tweets/CrisisLexT6/2013_Oklahoma_Tornado/2013_Oklahoma_Tornado-ontopic_offtopic.csv\")\n",
    "files.append(\"tweets/CrisisLexT6/2013_Queensland_Floods/2013_Queensland_Floods-ontopic_offtopic.csv\")\n",
    "files.append(\"tweets/CrisisLexT6/2013_West_Texas_Explosion/2013_West_Texas_Explosion-ontopic_offtopic.csv\")\n",
    "csvs_to_txt(files, out, \" tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Extract from ids\n",
    "def ids_to_txt(id_txt, output):\n",
    "    with open(id_txt, 'r') as in_file:\n",
    "        with open(output, 'w') as out_file:\n",
    "            count=0\n",
    "            count_t=0\n",
    "            t=2500\n",
    "            ids=[]\n",
    "            for line in in_file:\n",
    "                ids.append(int(line))\n",
    "                if len(ids)>=100:\n",
    "                    count_t+=100\n",
    "                    try:\n",
    "                        status=api.statuses_lookup(ids, False, True, False)\n",
    "                    except tweepy.TweepError:\n",
    "                        print \"Waiting for API Cooldown\"\n",
    "                        while(True):\n",
    "                            time.sleep(5 * 60)\n",
    "                            try:\n",
    "                                status=api.statuses_lookup(ids, False, True, False)\n",
    "                                break\n",
    "                            except tweepy.TweepError:\n",
    "                                continue\n",
    "                    count+=len(status)\n",
    "                    for s in status:\n",
    "                        out_file.write(s.text.encode('ascii', 'ignore').replace('\\r', '').replace('\\n',' ')+\"\\n\")\n",
    "                    ids=[]\n",
    "                    if count_t>=t:\n",
    "                        print str(count)+'/'+str(count_t)\n",
    "                        t+=2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Model\n",
    "from gensim.models import Word2Vec, FastText, KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors, FastTextKeyedVectors\n",
    "\n",
    "from glove import Glove, Corpus\n",
    "import re\n",
    "\n",
    "class TextIterator(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname=dirname\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                a=line.lower()\n",
    "                a=re.sub(r'https?:\\/\\/[^\\s]*','',a)\n",
    "                a=re.findall(r'\\w+(?:[-\\']\\w+)?',a)\n",
    "                yield a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CrisisLexT26\n",
    "\n",
    "out=\"tweets_CrisisLexT26.txt\"\n",
    "dirs = os.listdir('tweets/CrisisLexT26')\n",
    "dirs.remove('README.md')\n",
    "files = ['tweets/CrisisLexT26/'+n+\"/\"+n+\"-tweets_labeled.csv\" for n in dirs]\n",
    "csvs_to_txt(files, out, \" Tweet Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kaggle - Hurricane Harvey\n",
    "\n",
    "out=\"tweets_kaggle_harvey.txt\"\n",
    "files = ['tweets/hurricaneharvey/'+n for n in os.listdir('tweets/hurricaneharvey/')]\n",
    "\n",
    "# a=pd.read_table(files[1], sep=',')\n",
    "# a['Tweet'].head()[2]\n",
    "\n",
    "csvs_to_txt(files, out, \"Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISCRAM18_dataset - Harvey missing IDs\n",
    "out=\"tweets_ISCRAM18_harvey_missing_ids.txt\"\n",
    "files=['tweets/ISCRAM18_datasets/Harvey_missing_ids_50K.csv']\n",
    "csvs_to_txt(files, out, \"Tweet\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISCRAM2013_dataset\n",
    "out=\"tweets_ISCRAM2013.txt\"\n",
    "dirs=os.listdir('tweets/ISCRAM2013_dataset')\n",
    "dirs.remove('Terms of use.txt')\n",
    "dirs.remove('README.txt')\n",
    "files=['tweets/ISCRAM2013_dataset/'+n+\"/\"+os.listdir('tweets/ISCRAM2013_dataset/'+n)[0] for n in dirs]\n",
    "csvs_to_txt(files, out, \"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure-eight\n",
    "out=\"tweets_figure_eight.txt\"\n",
    "files=[\"tweets/socialmedia-disaster-tweets-DFE.csv\"]\n",
    "csvs_to_txt(files, out, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISCRAM18 datasets\n",
    "out=[\n",
    "    \"tweets_ISCRAM18_Harvey.txt\",\n",
    "    \"tweets_ISCRAM18_Irma.txt\",\n",
    "    \"tweets_ISCRAM18_Maria.txt\"\n",
    "]\n",
    "files=[\n",
    "    'tweets/ISCRAM18_datasets/Harvey_tweet_ids.txt',\n",
    "    'tweets/ISCRAM18_datasets/Irma_tweet_ids.txt',\n",
    "    'tweets/ISCRAM18_datasets/Maria_tweet_ids.txt'\n",
    "]\n",
    "for i, f in enumerate(files):\n",
    "    ids_to_txt(files[i], out[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927/2500\n",
      "3295/5000\n",
      "5053/7500\n",
      "6859/10000\n",
      "8085/12500\n",
      "9648/15000\n",
      "11338/17500\n",
      "12924/20000\n",
      "14402/22500\n",
      "16149/25000\n",
      "17761/27500\n",
      "19275/30000\n",
      "Waiting for API Cooldown\n",
      "20682/32500\n",
      "22060/35000\n",
      "23423/37500\n",
      "24715/40000\n",
      "26097/42500\n",
      "27538/45000\n",
      "28988/47500\n",
      "30377/50000\n",
      "31779/52500\n",
      "33157/55000\n",
      "34576/57500\n",
      "35970/60000\n",
      "37357/62500\n",
      "38761/65000\n",
      "40188/67500\n",
      "41559/70000\n",
      "42935/72500\n",
      "44303/75000\n",
      "45632/77500\n",
      "46967/80000\n",
      "48221/82500\n",
      "49555/85000\n",
      "50861/87500\n",
      "52072/90000\n",
      "53399/92500\n",
      "54748/95000\n",
      "55958/97500\n",
      "57218/100000\n",
      "58508/102500\n",
      "59842/105000\n",
      "61154/107500\n",
      "62511/110000\n",
      "63880/112500\n",
      "65305/115000\n",
      "66747/117500\n",
      "68239/120000\n",
      "Waiting for API Cooldown\n",
      "69637/122500\n",
      "71177/125000\n",
      "72787/127500\n",
      "74345/130000\n",
      "75818/132500\n",
      "77479/135000\n",
      "78990/137500\n",
      "80671/140000\n",
      "82320/142500\n",
      "83904/145000\n",
      "85513/147500\n",
      "86954/150000\n",
      "88527/152500\n",
      "90029/155000\n",
      "91617/157500\n",
      "93308/160000\n",
      "95030/162500\n",
      "96820/165000\n",
      "98566/167500\n",
      "100114/170000\n",
      "101817/172500\n",
      "103545/175000\n",
      "105403/177500\n",
      "107162/180000\n",
      "108855/182500\n",
      "110718/185000\n",
      "112578/187500\n",
      "114402/190000\n",
      "116160/192500\n",
      "117918/195000\n",
      "119417/197500\n",
      "120932/200000\n",
      "122584/202500\n",
      "124125/205000\n",
      "125826/207500\n",
      "127682/210000\n",
      "Waiting for API Cooldown\n",
      "129514/212500\n",
      "131422/215000\n",
      "133164/217500\n",
      "134630/220000\n",
      "136150/222500\n",
      "137418/225000\n",
      "139155/227500\n",
      "140541/230000\n",
      "142061/232500\n",
      "143584/235000\n",
      "145158/237500\n",
      "146882/240000\n",
      "148627/242500\n",
      "150329/245000\n",
      "151894/247500\n",
      "153449/250000\n",
      "155008/252500\n",
      "156687/255000\n",
      "158424/257500\n",
      "160043/260000\n",
      "161731/262500\n",
      "163487/265000\n",
      "165173/267500\n",
      "166923/270000\n",
      "168632/272500\n",
      "170390/275000\n",
      "172075/277500\n",
      "173640/280000\n",
      "175519/282500\n",
      "177367/285000\n"
     ]
    }
   ],
   "source": [
    "tdir=\"tweets/CrisisLexT26/\"\n",
    "ids=\"tweets/CL26_tweet_ids.txt\"\n",
    "out=\"tweets_CrisisLexT26_ids.txt\"\n",
    "dirs = os.listdir('tweets/CrisisLexT26')\n",
    "dirs.remove('README.md')\n",
    "files = ['tweets/CrisisLexT26/'+n+\"/\"+n+\"-tweetids_entire_period.csv\" for n in dirs]\n",
    "csvs_to_txt(files, ids, \" Tweet-ID\")\n",
    "ids_to_txt(ids,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"data/tweet_txts/\"\n",
    "sents=TextIterator(directory)\n",
    "\n",
    "model=Word2Vec(sents, size=300, workers=4, min_count=1)\n",
    "word_vectors=model.wv\n",
    "word_vectors.save(\"word2vec_tweets_300_2.wv\")\n",
    "# model=Word2Vec(sents, size=100, workers=4)\n",
    "# word_vectors=model.wv\n",
    "# word_vectors.save(\"word2vec_tweets_100.wv\")\n",
    "# model=Word2Vec(sents, size=500, workers=4)\n",
    "# word_vectors=model.wv\n",
    "# word_vectors.save(\"word2vec_tweets_500.wv\")\n",
    "# model=FastText(sents, size=300, workers=4)\n",
    "# word_vectors=model.wv\n",
    "# word_vectors.save(\"fasttext_300.wv\")\n",
    "# glove= Glove(no_components=300)\n",
    "# corpus=Corpus()\n",
    "# corpus.fit(sents, window=5)\n",
    "# #glove.fit(sents, epochs=5)\n",
    "\n",
    "# glove= Glove(no_components=300)\n",
    "# glove.fit(corpus.matrix, epochs=5, verbose=True)\n",
    "# glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "# glove.save('glove.wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_vec=KeyedVectors.load_word2vec_format(\n",
    "            'GoogleNews-vectors-negative300.bin', \n",
    "            binary=True, limit=200000)\n",
    "t_vec=Word2VecKeyedVectors.load(\"word2vec_tweets_300.wv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7321895\n",
      "0.40604332\n"
     ]
    }
   ],
   "source": [
    "w1, w2 = \"hurricane\", \"storm\"\n",
    "print g_vec.similarity(w1,w2)\n",
    "print t_vec.similarity(w1,w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
