{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Loading csv file and creating tweets and category list \n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold #import KFold\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors, FastTextKeyedVectors\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras import backend as K\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from glove import Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordVector(index):\n",
    "    if index==0:\n",
    "        google_vecs = KeyedVectors.load_word2vec_format(\n",
    "            '/home/hnakai/GoogleNews-vectors-negative300.bin', \n",
    "            binary=True, limit=200000)\n",
    "        return google_vecs\n",
    "    if index==1:\n",
    "        fasttext_vecs = FastTextKeyedVectors.load(\"fasttext_300.wv\")\n",
    "        return fasttext_vecs\n",
    "    elif index!=5:\n",
    "        wv_files=[\"word2vec_tweets_300.wv\",\"word2vec_tweets_100.wv\",\"word2vec_tweets_500.wv\"]\n",
    "        word_vec = Word2VecKeyedVectors.load(wv_files[index-2])\n",
    "        return word_vec\n",
    "    else: \n",
    "        glove = Glove.load('glove.wv')\n",
    "        word_vec = {}\n",
    "        for word, i in glove.dictionary.iteritems():\n",
    "            word_vec[word]=glove.word_vectors[i]\n",
    "        return word_vec\n",
    "\n",
    "filename = []\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\")\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2013_Alberta_Floods/2013_Alberta_Floods-ontopic_offtopic.csv\")\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2013_Boston_Bombings/2013_Boston_Bombings-ontopic_offtopic.csv\")\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2013_Oklahoma_Tornado/2013_Oklahoma_Tornado-ontopic_offtopic.csv\")\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2013_Queensland_Floods/2013_Queensland_Floods-ontopic_offtopic.csv\")\n",
    "filename.append(\"/home/hnakai/CrisisLexT6/2013_West_Texas_Explosion/2013_West_Texas_Explosion-ontopic_offtopic.csv\")\n",
    "\n",
    "def loadCsv(filename):\n",
    "    print(\"**************************************************************\")\n",
    "    print(\"\")\n",
    "    print(\"File Name: \"+filename)\n",
    "    print(\"\")\n",
    "    lines = csv.reader(open(filename, 'rt'))\n",
    "    dataset = list(lines)\n",
    "    nltk_tweets = []\n",
    "    tweets = []\n",
    "    filtered_tweets = []\n",
    "    full_category = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        if i>0:\n",
    "            nltk_tweets.append((dataset[i][1], dataset[i][2])) #array containing both tweets and category\n",
    "            tweets.append(dataset[i][1]) #array containing just tweets\n",
    "            full_category.append(dataset[i][2]) #array containing just category\n",
    "\n",
    "    return nltk_tweets,tweets,full_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation - 5-Fold Validation\n",
    "\n",
    "\n",
    "def crossvalidate(data):\n",
    "    x = np.array(data) #convert tweets into np array\n",
    "    kf = KFold(n_splits=5) # implementing 5-fold validation\n",
    "    kf.get_n_splits(x) # returns the number of splitting iterations in cross-validator\n",
    "    return kf,x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "\n",
    "\n",
    "# Parameters\n",
    "dropout = 0.25\n",
    "learning_rate = 0.00005 \n",
    "n_hidden = 3\n",
    "n_epochs = 50\n",
    "min_cc=5\n",
    "\n",
    "def fit_and_predict_w2v(kf, x, classifier, tweets, full_category, wv=1):\n",
    "    average = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        train_set = []\n",
    "        test_set = []\n",
    "        categories_train = []\n",
    "        categories_test = []\n",
    "        for index in train_index:\n",
    "            train_set.append(tweets[index])\n",
    "            categories_train.append(0 if full_category[index]=='off-topic' else 1)\n",
    "        for ind in test_index:\n",
    "            test_set.append(tweets[ind])\n",
    "            categories_test.append(0 if full_category[ind]=='off-topic' else 1)\n",
    "        tweet_train_counts, categories_train = word2vec(train_set, categories_train, wv)\n",
    "        tweet_test_counts, categories_test = word2vec(test_set, categories_test, wv)\n",
    "        print(tweet_train_counts.shape)\n",
    "        n_input = tweet_train_counts.shape[1]\n",
    "        \n",
    "        classifier = classifier.fit(tweet_train_counts, categories_train)\n",
    "        acc = accuracy_score(categories_test, classifier.predict(tweet_test_counts))\n",
    "        print acc\n",
    "        average.append(acc)\n",
    "    \n",
    "    sum = 0\n",
    "    for value in average:\n",
    "        sum = sum + value\n",
    "    print(\"Accuracy: \")\n",
    "    print((sum/5)*100)\n",
    "    \n",
    "    return sum/5*100\n",
    "\n",
    "def create_model(n_input):\n",
    "    model=Sequential()\n",
    "    n_neurons = int(math.ceil(float(n_input)*2.0/3.0))\n",
    "    model.add(Dense(n_neurons, activation='relu', input_dim=n_input))\n",
    "    model.add(Dropout(dropout))\n",
    "    for _ in range(1, n_hidden):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', \n",
    "        optimizer=RMSprop(lr=learning_rate),\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit_and_predict_w2v_nn(kf, x, tweets, full_category, wv=1):\n",
    "    average = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(x):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        train_set = []\n",
    "        test_set = []\n",
    "        categories_train = []\n",
    "        categories_test = []\n",
    "        for index in train_index:\n",
    "            train_set.append(tweets[index])\n",
    "            categories_train.append(0 if full_category[index]=='off-topic' else 1)\n",
    "        for ind in test_index:\n",
    "            test_set.append(tweets[ind])\n",
    "            categories_test.append(0 if full_category[ind]=='off-topic' else 1)\n",
    "        tweet_train_counts, categories_train = word2vec(train_set, categories_train, wv)\n",
    "        tweet_test_counts, categories_test = word2vec(test_set, categories_test, wv)\n",
    "        print(tweet_train_counts.shape)\n",
    "        n_input = tweet_train_counts.shape[1]\n",
    "        \n",
    "        #Reinitialize TensorFlow: due to bug in TF\n",
    "        tf.reset_default_graph()\n",
    "        K.clear_session()\n",
    "        K.set_session(tf.Session())\n",
    "        \n",
    "        model = create_model(n_input)\n",
    "        model.fit(tweet_train_counts, categories_train, epochs=n_epochs, batch_size=50, verbose=0)\n",
    "        acc = model.evaluate(tweet_test_counts, categories_test, batch_size=50, verbose=0)[1]\n",
    "        print acc\n",
    "        average.append(acc)\n",
    "    \n",
    "    sum = 0\n",
    "    for value in average:\n",
    "        sum = sum + value\n",
    "    print(\"Accuracy: \")\n",
    "    print((sum/5)*100)\n",
    "    \n",
    "    return sum/5*100\n",
    "    \n",
    "def word2vec(texts, related, wv=1, pos_filter = \"nofilter\"):\n",
    "    vects, voided = [], []\n",
    "    tokenizer = RegexpTokenizer('(?:@?)+\\w+(?:(?:\\'|-)\\w+)?')\n",
    "    tokens = [[w for w in tokenizer.tokenize(d.lower()) \n",
    "               if (pos_filter == \"nofilter\"\n",
    "                   or (pos_filter == \"stopwords\" and w not in stop_words)\n",
    "                   or map_tag('en-ptb', 'universal', pos_tag([w])[0][1]) in pos_filter\n",
    "                  )]\n",
    "              for d in texts]\n",
    "    word_vec=loadWordVector(wv)\n",
    "    for i, d in enumerate(tokens):\n",
    "        count = 0\n",
    "        doc_vect = np.zeros(len(word_vec[\"hurricane\"]))\n",
    "        for w in d:\n",
    "            try:\n",
    "                doc_vect += word_vec[w.decode('ascii', 'ignore')]\n",
    "                count+=1\n",
    "            except KeyError:\n",
    "                continue\n",
    "        if np.isnan(np.min(doc_vect)) or count == 0:\n",
    "            voided.append(i)\n",
    "            continue\n",
    "        doc_vect /= count\n",
    "        vects.append(doc_vect)\n",
    "    return np.array(vects), np.delete(related, voided, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2012_Sandy_Hurricane/2012_Sandy_Hurricane-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10005, 10006, 10007]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8006, 300)\n",
      "0.7182817182817183\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([2002, 2003, 2004, ..., 4001, 4002, 4003]))\n",
      "(8006, 300)\n",
      "0.7387612387612388\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([4004, 4005, 4006, ..., 6003, 6004, 6005]))\n",
      "(8006, 300)\n",
      "0.7737262737262737\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([6006, 6007, 6008, ..., 8004, 8005, 8006]))\n",
      "(8007, 300)\n",
      "0.8700649675162418\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8004, 8005, 8006]), 'TEST:', array([ 8007,  8008,  8009, ..., 10005, 10006, 10007]))\n",
      "(8007, 300)\n",
      "0.856071964017991\n",
      "Accuracy: \n",
      "79.13812324606928\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10005, 10006, 10007]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8006, 300)\n",
      "0.7872127872127872\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([2002, 2003, 2004, ..., 4001, 4002, 4003]))\n",
      "(8006, 300)\n",
      "0.8051948051948052\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([4004, 4005, 4006, ..., 6003, 6004, 6005]))\n",
      "(8006, 300)\n",
      "0.8716283716283716\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([6006, 6007, 6008, ..., 8004, 8005, 8006]))\n",
      "(8007, 300)\n",
      "0.9245377311344328\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8004, 8005, 8006]), 'TEST:', array([ 8007,  8008,  8009, ..., 10005, 10006, 10007]))\n",
      "(8007, 300)\n",
      "0.9425287356321839\n",
      "Accuracy: \n",
      "86.62204861605161\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10005, 10006, 10007]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8006, 300)\n",
      "0.7717282717282717\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([2002, 2003, 2004, ..., 4001, 4002, 4003]))\n",
      "(8006, 300)\n",
      "0.7927072927072927\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([4004, 4005, 4006, ..., 6003, 6004, 6005]))\n",
      "(8006, 300)\n",
      "0.8551448551448552\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([6006, 6007, 6008, ..., 8004, 8005, 8006]))\n",
      "(8007, 300)\n",
      "0.9280359820089955\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8004, 8005, 8006]), 'TEST:', array([ 8007,  8008,  8009, ..., 10005, 10006, 10007]))\n",
      "(8007, 300)\n",
      "0.9505247376311844\n",
      "Accuracy: \n",
      "85.962822784412\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10005, 10006, 10007]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8006, 300)\n",
      "0.8201798175002907\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([2002, 2003, 2004, ..., 4001, 4002, 4003]))\n",
      "(8006, 300)\n",
      "0.831668329465163\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([4004, 4005, 4006, ..., 6003, 6004, 6005]))\n",
      "(8006, 300)\n",
      "0.8856143815653188\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10005, 10006, 10007]), 'TEST:', array([6006, 6007, 6008, ..., 8004, 8005, 8006]))\n",
      "(8007, 300)\n",
      "0.9400299870926162\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8004, 8005, 8006]), 'TEST:', array([ 8007,  8008,  8009, ..., 10005, 10006, 10007]))\n",
      "(8007, 300)\n",
      "0.9625187376509423\n",
      "Accuracy: \n",
      "88.80022506548663\n",
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2013_Alberta_Floods/2013_Alberta_Floods-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10028, 10029, 10030]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8024, 300)\n",
      "0.7872446437468859\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([2007, 2008, 2009, ..., 4010, 4011, 4012]))\n",
      "(8025, 300)\n",
      "0.8010967098703888\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([4013, 4014, 4015, ..., 6016, 6017, 6018]))\n",
      "(8025, 300)\n",
      "0.8245264207377866\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([6019, 6020, 6021, ..., 8022, 8023, 8024]))\n",
      "(8025, 300)\n",
      "0.8763708873379861\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8022, 8023, 8024]), 'TEST:', array([ 8025,  8026,  8027, ..., 10028, 10029, 10030]))\n",
      "(8025, 300)\n",
      "0.8743768693918246\n",
      "Accuracy: \n",
      "83.27231062169744\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10028, 10029, 10030]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8024, 300)\n",
      "0.8873941205779771\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([2007, 2008, 2009, ..., 4010, 4011, 4012]))\n",
      "(8025, 300)\n",
      "0.8968095712861416\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([4013, 4014, 4015, ..., 6016, 6017, 6018]))\n",
      "(8025, 300)\n",
      "0.8923230309072782\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([6019, 6020, 6021, ..., 8022, 8023, 8024]))\n",
      "(8025, 300)\n",
      "0.8678963110667997\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8022, 8023, 8024]), 'TEST:', array([ 8025,  8026,  8027, ..., 10028, 10029, 10030]))\n",
      "(8025, 300)\n",
      "0.8788634097706879\n",
      "Accuracy: \n",
      "88.46572887217768\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10028, 10029, 10030]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8024, 300)\n",
      "0.9068261086198306\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([2007, 2008, 2009, ..., 4010, 4011, 4012]))\n",
      "(8025, 300)\n",
      "0.8858424725822532\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([4013, 4014, 4015, ..., 6016, 6017, 6018]))\n",
      "(8025, 300)\n",
      "0.8893320039880359\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([6019, 6020, 6021, ..., 8022, 8023, 8024]))\n",
      "(8025, 300)\n",
      "0.8883349950149552\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8022, 8023, 8024]), 'TEST:', array([ 8025,  8026,  8027, ..., 10028, 10029, 10030]))\n",
      "(8025, 300)\n",
      "0.8718843469591226\n",
      "Accuracy: \n",
      "88.84439854328396\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10028, 10029, 10030]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8024, 300)\n",
      "0.9217737891154915\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([2007, 2008, 2009, ..., 4010, 4011, 4012]))\n",
      "(8025, 300)\n",
      "0.9162512414476808\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([4013, 4014, 4015, ..., 6016, 6017, 6018]))\n",
      "(8025, 300)\n",
      "0.9232303080031071\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10028, 10029, 10030]), 'TEST:', array([6019, 6020, 6021, ..., 8022, 8023, 8024]))\n",
      "(8025, 300)\n",
      "0.9007976081887128\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8022, 8023, 8024]), 'TEST:', array([ 8025,  8026,  8027, ..., 10028, 10029, 10030]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8025, 300)\n",
      "0.9257228295444134\n",
      "Accuracy: \n",
      "91.7555155259881\n",
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2013_Boston_Bombings/2013_Boston_Bombings-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2003,  2004,  2005, ..., 10009, 10010, 10011]), 'TEST:', array([   0,    1,    2, ..., 2000, 2001, 2002]))\n",
      "(8009, 300)\n",
      "0.8262606090863704\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([2003, 2004, 2005, ..., 4003, 4004, 4005]))\n",
      "(8009, 300)\n",
      "0.8227658512231653\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([4006, 4007, 4008, ..., 6005, 6006, 6007]))\n",
      "(8010, 300)\n",
      "0.8396603396603397\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([6008, 6009, 6010, ..., 8007, 8008, 8009]))\n",
      "(8010, 300)\n",
      "0.8666333666333667\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8007, 8008, 8009]), 'TEST:', array([ 8010,  8011,  8012, ..., 10009, 10010, 10011]))\n",
      "(8010, 300)\n",
      "0.8731268731268731\n",
      "Accuracy: \n",
      "84.56894079460231\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2003,  2004,  2005, ..., 10009, 10010, 10011]), 'TEST:', array([   0,    1,    2, ..., 2000, 2001, 2002]))\n",
      "(8009, 300)\n",
      "0.8761857214178732\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([2003, 2004, 2005, ..., 4003, 4004, 4005]))\n",
      "(8009, 300)\n",
      "0.8612081877184223\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([4006, 4007, 4008, ..., 6005, 6006, 6007]))\n",
      "(8010, 300)\n",
      "0.9010989010989011\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([6008, 6009, 6010, ..., 8007, 8008, 8009]))\n",
      "(8010, 300)\n",
      "0.9245754245754245\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8007, 8008, 8009]), 'TEST:', array([ 8010,  8011,  8012, ..., 10009, 10010, 10011]))\n",
      "(8010, 300)\n",
      "0.9305694305694305\n",
      "Accuracy: \n",
      "89.87275330760103\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2003,  2004,  2005, ..., 10009, 10010, 10011]), 'TEST:', array([   0,    1,    2, ..., 2000, 2001, 2002]))\n",
      "(8009, 300)\n",
      "0.8701947079380928\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([2003, 2004, 2005, ..., 4003, 4004, 4005]))\n",
      "(8009, 300)\n",
      "0.8512231652521218\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([4006, 4007, 4008, ..., 6005, 6006, 6007]))\n",
      "(8010, 300)\n",
      "0.8766233766233766\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([6008, 6009, 6010, ..., 8007, 8008, 8009]))\n",
      "(8010, 300)\n",
      "0.9465534465534465\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8007, 8008, 8009]), 'TEST:', array([ 8010,  8011,  8012, ..., 10009, 10010, 10011]))\n",
      "(8010, 300)\n",
      "0.952047952047952\n",
      "Accuracy: \n",
      "89.9328529682998\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2003,  2004,  2005, ..., 10009, 10010, 10011]), 'TEST:', array([   0,    1,    2, ..., 2000, 2001, 2002]))\n",
      "(8009, 300)\n",
      "0.8946580107784604\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([2003, 2004, 2005, ..., 4003, 4004, 4005]))\n",
      "(8009, 300)\n",
      "0.8806789748024715\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([4006, 4007, 4008, ..., 6005, 6006, 6007]))\n",
      "(8010, 300)\n",
      "0.9205794174235303\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10009, 10010, 10011]), 'TEST:', array([6008, 6009, 6010, ..., 8007, 8008, 8009]))\n",
      "(8010, 300)\n",
      "0.9430569403178685\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8007, 8008, 8009]), 'TEST:', array([ 8010,  8011,  8012, ..., 10009, 10010, 10011]))\n",
      "(8010, 300)\n",
      "0.9565434577939036\n",
      "Accuracy: \n",
      "91.91033602232469\n",
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2013_Oklahoma_Tornado/2013_Oklahoma_Tornado-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([1999, 2000, 2001, ..., 9989, 9990, 9991]), 'TEST:', array([   0,    1,    2, ..., 1996, 1997, 1998]))\n",
      "(7993, 300)\n",
      "0.7898949474737369\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([1999, 2000, 2001, ..., 3995, 3996, 3997]))\n",
      "(7993, 300)\n",
      "0.806903451725863\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([3998, 3999, 4000, ..., 5993, 5994, 5995]))\n",
      "(7994, 300)\n",
      "0.8038038038038038\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([5996, 5997, 5998, ..., 7991, 7992, 7993]))\n",
      "(7994, 300)\n",
      "0.8173173173173173\n",
      "('TRAIN:', array([   0,    1,    2, ..., 7991, 7992, 7993]), 'TEST:', array([7994, 7995, 7996, ..., 9989, 9990, 9991]))\n",
      "(7994, 300)\n",
      "0.8338338338338338\n",
      "Accuracy: \n",
      "81.03506708309108\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([1999, 2000, 2001, ..., 9989, 9990, 9991]), 'TEST:', array([   0,    1,    2, ..., 1996, 1997, 1998]))\n",
      "(7993, 300)\n",
      "0.8574287143571786\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([1999, 2000, 2001, ..., 3995, 3996, 3997]))\n",
      "(7993, 300)\n",
      "0.8669334667333667\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([3998, 3999, 4000, ..., 5993, 5994, 5995]))\n",
      "(7994, 300)\n",
      "0.8528528528528528\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([5996, 5997, 5998, ..., 7991, 7992, 7993]))\n",
      "(7994, 300)\n",
      "0.7922922922922923\n",
      "('TRAIN:', array([   0,    1,    2, ..., 7991, 7992, 7993]), 'TEST:', array([7994, 7995, 7996, ..., 9989, 9990, 9991]))\n",
      "(7994, 300)\n",
      "0.7972972972972973\n",
      "Accuracy: \n",
      "83.33609247065976\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([1999, 2000, 2001, ..., 9989, 9990, 9991]), 'TEST:', array([   0,    1,    2, ..., 1996, 1997, 1998]))\n",
      "(7993, 300)\n",
      "0.7468734367183592\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([1999, 2000, 2001, ..., 3995, 3996, 3997]))\n",
      "(7993, 300)\n",
      "0.8544272136068034\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([3998, 3999, 4000, ..., 5993, 5994, 5995]))\n",
      "(7994, 300)\n",
      "0.8363363363363363\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([5996, 5997, 5998, ..., 7991, 7992, 7993]))\n",
      "(7994, 300)\n",
      "0.8078078078078078\n",
      "('TRAIN:', array([   0,    1,    2, ..., 7991, 7992, 7993]), 'TEST:', array([7994, 7995, 7996, ..., 9989, 9990, 9991]))\n",
      "(7994, 300)\n",
      "0.8578578578578578\n",
      "Accuracy: \n",
      "82.06605304654329\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([1999, 2000, 2001, ..., 9989, 9990, 9991]), 'TEST:', array([   0,    1,    2, ..., 1996, 1997, 1998]))\n",
      "(7993, 300)\n",
      "0.8964482213390536\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([1999, 2000, 2001, ..., 3995, 3996, 3997]))\n",
      "(7993, 300)\n",
      "0.8959479713630771\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([3998, 3999, 4000, ..., 5993, 5994, 5995]))\n",
      "(7994, 300)\n",
      "0.8743743735390741\n",
      "('TRAIN:', array([   0,    1,    2, ..., 9989, 9990, 9991]), 'TEST:', array([5996, 5997, 5998, ..., 7991, 7992, 7993]))\n",
      "(7994, 300)\n",
      "0.8528528471847435\n",
      "('TRAIN:', array([   0,    1,    2, ..., 7991, 7992, 7993]), 'TEST:', array([7994, 7995, 7996, ..., 9989, 9990, 9991]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7994, 300)\n",
      "0.8633633596641762\n",
      "Accuracy: \n",
      "87.65973546180248\n",
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2013_Queensland_Floods/2013_Queensland_Floods-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10030, 10031, 10032]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8026, 300)\n",
      "0.8121574489287494\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([2007, 2008, 2009, ..., 4011, 4012, 4013]))\n",
      "(8026, 300)\n",
      "0.8290981564524166\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([4014, 4015, 4016, ..., 6018, 6019, 6020]))\n",
      "(8026, 300)\n",
      "0.8704534130543099\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([6021, 6022, 6023, ..., 8024, 8025, 8026]))\n",
      "(8027, 300)\n",
      "0.922233300099701\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8024, 8025, 8026]), 'TEST:', array([ 8027,  8028,  8029, ..., 10030, 10031, 10032]))\n",
      "(8027, 300)\n",
      "0.9232303090727817\n",
      "Accuracy: \n",
      "87.14345255215918\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10030, 10031, 10032]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8026, 300)\n",
      "0.8923766816143498\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([2007, 2008, 2009, ..., 4011, 4012, 4013]))\n",
      "(8026, 300)\n",
      "0.8829098156452416\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([4014, 4015, 4016, ..., 6018, 6019, 6020]))\n",
      "(8026, 300)\n",
      "0.9312406576980568\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([6021, 6022, 6023, ..., 8024, 8025, 8026]))\n",
      "(8027, 300)\n",
      "0.959122632103689\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8024, 8025, 8026]), 'TEST:', array([ 8027,  8028,  8029, ..., 10030, 10031, 10032]))\n",
      "(8027, 300)\n",
      "0.963110667996012\n",
      "Accuracy: \n",
      "92.57520910114698\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10030, 10031, 10032]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8026, 300)\n",
      "0.8475336322869955\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([2007, 2008, 2009, ..., 4011, 4012, 4013]))\n",
      "(8026, 300)\n",
      "0.8415545590433483\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([4014, 4015, 4016, ..., 6018, 6019, 6020]))\n",
      "(8026, 300)\n",
      "0.9277528649725959\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([6021, 6022, 6023, ..., 8024, 8025, 8026]))\n",
      "(8027, 300)\n",
      "0.9685942173479561\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8024, 8025, 8026]), 'TEST:', array([ 8027,  8028,  8029, ..., 10030, 10031, 10032]))\n",
      "(8027, 300)\n",
      "0.9680957128614157\n",
      "Accuracy: \n",
      "91.07061973024624\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2007,  2008,  2009, ..., 10030, 10031, 10032]), 'TEST:', array([   0,    1,    2, ..., 2004, 2005, 2006]))\n",
      "(8026, 300)\n",
      "0.9227702974025801\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([2007, 2008, 2009, ..., 4011, 4012, 4013]))\n",
      "(8026, 300)\n",
      "0.9177877406724722\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([4014, 4015, 4016, ..., 6018, 6019, 6020]))\n",
      "(8026, 300)\n",
      "0.9551569518605809\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10030, 10031, 10032]), 'TEST:', array([6021, 6022, 6023, ..., 8024, 8025, 8026]))\n",
      "(8027, 300)\n",
      "0.9830508487650071\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8024, 8025, 8026]), 'TEST:', array([ 8027,  8028,  8029, ..., 10030, 10031, 10032]))\n",
      "(8027, 300)\n",
      "0.9815553367316188\n",
      "Accuracy: \n",
      "95.20642350864519\n",
      "**************************************************************\n",
      "\n",
      "File Name: /home/hnakai/CrisisLexT6/2013_West_Texas_Explosion/2013_West_Texas_Explosion-ontopic_offtopic.csv\n",
      "\n",
      "**************************************************************\n",
      "Naive Bayes Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10003, 10004, 10005]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8004, 300)\n",
      "0.8951048951048951\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([2002, 2003, 2004, ..., 4000, 4001, 4002]))\n",
      "(8005, 300)\n",
      "0.9045477261369316\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([4003, 4004, 4005, ..., 6001, 6002, 6003]))\n",
      "(8005, 300)\n",
      "0.9150424787606197\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([6004, 6005, 6006, ..., 8002, 8003, 8004]))\n",
      "(8005, 300)\n",
      "0.9330334832583708\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8002, 8003, 8004]), 'TEST:', array([ 8005,  8006,  8007, ..., 10003, 10004, 10005]))\n",
      "(8005, 300)\n",
      "0.9315342328835582\n",
      "Accuracy: \n",
      "91.5852563228875\n",
      "**************************************************************\n",
      "Logistic Regression Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10003, 10004, 10005]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8004, 300)\n",
      "0.9205794205794205\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([2002, 2003, 2004, ..., 4000, 4001, 4002]))\n",
      "(8005, 300)\n",
      "0.9190404797601199\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([4003, 4004, 4005, ..., 6001, 6002, 6003]))\n",
      "(8005, 300)\n",
      "0.9410294852573713\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([6004, 6005, 6006, ..., 8002, 8003, 8004]))\n",
      "(8005, 300)\n",
      "0.9485257371314343\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8002, 8003, 8004]), 'TEST:', array([ 8005,  8006,  8007, ..., 10003, 10004, 10005]))\n",
      "(8005, 300)\n",
      "0.9415292353823088\n",
      "Accuracy: \n",
      "93.4140871622131\n",
      "**************************************************************\n",
      "Support Vector Machine Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10003, 10004, 10005]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8004, 300)\n",
      "0.9280719280719281\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([2002, 2003, 2004, ..., 4000, 4001, 4002]))\n",
      "(8005, 300)\n",
      "0.9170414792603698\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([4003, 4004, 4005, ..., 6001, 6002, 6003]))\n",
      "(8005, 300)\n",
      "0.9305347326336831\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([6004, 6005, 6006, ..., 8002, 8003, 8004]))\n",
      "(8005, 300)\n",
      "0.9410294852573713\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8002, 8003, 8004]), 'TEST:', array([ 8005,  8006,  8007, ..., 10003, 10004, 10005]))\n",
      "(8005, 300)\n",
      "0.9330334832583708\n",
      "Accuracy: \n",
      "92.99422216963447\n",
      "**************************************************************\n",
      "Neural Network Classification using Word2Vec Word Embeddings\n",
      "**************************************************************\n",
      "('TRAIN:', array([ 2002,  2003,  2004, ..., 10003, 10004, 10005]), 'TEST:', array([   0,    1,    2, ..., 1999, 2000, 2001]))\n",
      "(8004, 300)\n",
      "0.9455544423985552\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([2002, 2003, 2004, ..., 4000, 4001, 4002]))\n",
      "(8005, 300)\n",
      "0.9375312314040657\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([4003, 4004, 4005, ..., 6001, 6002, 6003]))\n",
      "(8005, 300)\n",
      "0.9575212396781841\n",
      "('TRAIN:', array([    0,     1,     2, ..., 10003, 10004, 10005]), 'TEST:', array([6004, 6005, 6006, ..., 8002, 8003, 8004]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8005, 300)\n",
      "0.9695152461320266\n",
      "('TRAIN:', array([   0,    1,    2, ..., 8002, 8003, 8004]), 'TEST:', array([ 8005,  8006,  8007, ..., 10003, 10004, 10005]))\n",
      "(8005, 300)\n",
      "0.9775112464629311\n",
      "Accuracy: \n",
      "95.75266812151526\n"
     ]
    }
   ],
   "source": [
    "def classify_crisis_tweets_w2v(wv=1):\n",
    "    accuracies = []\n",
    "    \n",
    "    for i, file in enumerate(filename):\n",
    "        nltk_tweets= []\n",
    "        tweets = []\n",
    "        full_category = []\n",
    "        nltk_tweets, tweets, full_category = loadCsv(file)\n",
    "        kf,x = crossvalidate(nltk_tweets)\n",
    "        \n",
    "        accs = []\n",
    "        \n",
    "        names = [\"Naive Bayes\", \"Logistic Regression\", \"Support Vector Machine\"]\n",
    "        for i, classifier in enumerate([GaussianNB(),\n",
    "                                        LogisticRegression(),\n",
    "                                        SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, max_iter=5, random_state=42)\n",
    "                                       ]):\n",
    "            print(\"**************************************************************\")\n",
    "            print(names[i]+\" Classification using Word2Vec Word Embeddings\")\n",
    "            print(\"**************************************************************\")\n",
    "            accs.append(fit_and_predict_w2v(kf, x, classifier, tweets, full_category, wv))\n",
    "        \n",
    "        print(\"**************************************************************\")\n",
    "        print(\"Neural Network Classification using Word2Vec Word Embeddings\")\n",
    "        print(\"**************************************************************\")\n",
    "        accs.append(fit_and_predict_w2v_nn(kf, x, tweets, full_category, wv))\n",
    "        accuracies.append(accs)\n",
    "    return accuracies\n",
    "accs=[]\n",
    "wvn=[\"google\",\"fasttext\",\"tw300\",\"tw100\",\"tw500\",\"glove\"]\n",
    "for i in range(6):\n",
    "    if \"w2v_comparison_\"+wvn[i]+\".csv\" in os.listdir('.'):\n",
    "        accs.append(None)\n",
    "    else:\n",
    "        accuracies = classify_crisis_tweets_w2v(i)\n",
    "        accs.append(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "corpus.append(\"sandy\")\n",
    "corpus.append(\"alberta\")\n",
    "corpus.append(\"boston\")\n",
    "corpus.append(\"oklahoma\")\n",
    "corpus.append(\"queensland\")\n",
    "corpus.append(\"west_texas\")\n",
    "\n",
    "wvn=[\"google\",\"fasttext\",\"tw300\",\"tw100\",\"tw500\",\"glove\"]\n",
    "\n",
    "df_accs=[]\n",
    "for i, a in enumerate(accs):\n",
    "    if \"w2v_comparison_\"+wvn[i]+\".csv\" in os.listdir('.'):\n",
    "        df_accs.append(pd.read_csv(\"w2v_comparison_\"+wvn[i]+\".csv\", header=0))\n",
    "    else:\n",
    "        df_accs.append(pd.DataFrame(a,\n",
    "                     columns=[\"Naive Bayes\",\"Logistic Regression\", \"SVM\", \"Neural Network\"],\n",
    "                     index = corpus\n",
    "                    ))\n",
    "        df_accs[-1].to_csv(\"w2v_comparison_\"+wvn[i]+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandy</td>\n",
       "      <td>87.650700</td>\n",
       "      <td>90.408437</td>\n",
       "      <td>89.649241</td>\n",
       "      <td>91.137857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alberta</td>\n",
       "      <td>81.686748</td>\n",
       "      <td>85.205678</td>\n",
       "      <td>82.872697</td>\n",
       "      <td>86.930483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boston</td>\n",
       "      <td>82.821263</td>\n",
       "      <td>86.646599</td>\n",
       "      <td>85.977687</td>\n",
       "      <td>88.065060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>86.448495</td>\n",
       "      <td>89.731207</td>\n",
       "      <td>90.451903</td>\n",
       "      <td>91.813154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queensland</td>\n",
       "      <td>92.166232</td>\n",
       "      <td>95.405547</td>\n",
       "      <td>95.016868</td>\n",
       "      <td>96.372447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west_texas</td>\n",
       "      <td>92.334552</td>\n",
       "      <td>94.553278</td>\n",
       "      <td>94.163363</td>\n",
       "      <td>95.952493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "0       sandy    87.650700            90.408437  89.649241       91.137857\n",
       "1     alberta    81.686748            85.205678  82.872697       86.930483\n",
       "2      boston    82.821263            86.646599  85.977687       88.065060\n",
       "3    oklahoma    86.448495            89.731207  90.451903       91.813154\n",
       "4  queensland    92.166232            95.405547  95.016868       96.372447\n",
       "5  west_texas    92.334552            94.553278  94.163363       95.952493"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Google\n",
    "df_accs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandy</td>\n",
       "      <td>73.193084</td>\n",
       "      <td>88.819726</td>\n",
       "      <td>85.303382</td>\n",
       "      <td>89.109546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alberta</td>\n",
       "      <td>78.686819</td>\n",
       "      <td>92.662610</td>\n",
       "      <td>89.840975</td>\n",
       "      <td>93.250840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boston</td>\n",
       "      <td>78.657334</td>\n",
       "      <td>90.112558</td>\n",
       "      <td>88.514237</td>\n",
       "      <td>91.081345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>81.185112</td>\n",
       "      <td>90.702198</td>\n",
       "      <td>87.430467</td>\n",
       "      <td>91.552858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queensland</td>\n",
       "      <td>84.073748</td>\n",
       "      <td>95.415666</td>\n",
       "      <td>95.226264</td>\n",
       "      <td>96.033563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west_texas</td>\n",
       "      <td>89.656520</td>\n",
       "      <td>95.572853</td>\n",
       "      <td>90.755137</td>\n",
       "      <td>96.841959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "0       sandy    73.193084            88.819726  85.303382       89.109546\n",
       "1     alberta    78.686819            92.662610  89.840975       93.250840\n",
       "2      boston    78.657334            90.112558  88.514237       91.081345\n",
       "3    oklahoma    81.185112            90.702198  87.430467       91.552858\n",
       "4  queensland    84.073748            95.415666  95.226264       96.033563\n",
       "5  west_texas    89.656520            95.572853  90.755137       96.841959"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FastText\n",
    "df_accs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandy</td>\n",
       "      <td>80.436725</td>\n",
       "      <td>89.649196</td>\n",
       "      <td>85.812104</td>\n",
       "      <td>90.978032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alberta</td>\n",
       "      <td>81.338307</td>\n",
       "      <td>91.994679</td>\n",
       "      <td>90.648647</td>\n",
       "      <td>93.430287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boston</td>\n",
       "      <td>81.483384</td>\n",
       "      <td>90.632059</td>\n",
       "      <td>90.691775</td>\n",
       "      <td>91.980341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>84.307068</td>\n",
       "      <td>90.561978</td>\n",
       "      <td>90.372238</td>\n",
       "      <td>92.483544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queensland</td>\n",
       "      <td>87.522127</td>\n",
       "      <td>95.196424</td>\n",
       "      <td>94.608576</td>\n",
       "      <td>96.183095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west_texas</td>\n",
       "      <td>92.374742</td>\n",
       "      <td>95.812698</td>\n",
       "      <td>95.352978</td>\n",
       "      <td>96.851924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "0       sandy    80.436725            89.649196  85.812104       90.978032\n",
       "1     alberta    81.338307            91.994679  90.648647       93.430287\n",
       "2      boston    81.483384            90.632059  90.691775       91.980341\n",
       "3    oklahoma    84.307068            90.561978  90.372238       92.483544\n",
       "4  queensland    87.522127            95.196424  94.608576       96.183095\n",
       "5  west_texas    92.374742            95.812698  95.352978       96.851924"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W2V 300\n",
    "df_accs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandy</td>\n",
       "      <td>78.058573</td>\n",
       "      <td>87.151240</td>\n",
       "      <td>84.653562</td>\n",
       "      <td>87.571004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alberta</td>\n",
       "      <td>80.859792</td>\n",
       "      <td>88.884324</td>\n",
       "      <td>86.262001</td>\n",
       "      <td>88.914214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boston</td>\n",
       "      <td>80.124972</td>\n",
       "      <td>88.065170</td>\n",
       "      <td>86.796533</td>\n",
       "      <td>89.153682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>83.416373</td>\n",
       "      <td>89.361042</td>\n",
       "      <td>88.410787</td>\n",
       "      <td>90.131692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queensland</td>\n",
       "      <td>87.980523</td>\n",
       "      <td>94.747889</td>\n",
       "      <td>93.871107</td>\n",
       "      <td>95.186389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west_texas</td>\n",
       "      <td>91.695151</td>\n",
       "      <td>94.933168</td>\n",
       "      <td>93.054142</td>\n",
       "      <td>95.332903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "0       sandy    78.058573            87.151240  84.653562       87.571004\n",
       "1     alberta    80.859792            88.884324  86.262001       88.914214\n",
       "2      boston    80.124972            88.065170  86.796533       89.153682\n",
       "3    oklahoma    83.416373            89.361042  88.410787       90.131692\n",
       "4  queensland    87.980523            94.747889  93.871107       95.186389\n",
       "5  west_texas    91.695151            94.933168  93.054142       95.332903"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W2V 100\n",
    "df_accs[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sandy</td>\n",
       "      <td>80.626510</td>\n",
       "      <td>89.699062</td>\n",
       "      <td>86.721744</td>\n",
       "      <td>90.908122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alberta</td>\n",
       "      <td>81.707195</td>\n",
       "      <td>92.493163</td>\n",
       "      <td>91.286594</td>\n",
       "      <td>93.649545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boston</td>\n",
       "      <td>81.553379</td>\n",
       "      <td>91.001604</td>\n",
       "      <td>91.051350</td>\n",
       "      <td>92.469737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>oklahoma</td>\n",
       "      <td>84.367164</td>\n",
       "      <td>90.432033</td>\n",
       "      <td>90.962503</td>\n",
       "      <td>92.363534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>queensland</td>\n",
       "      <td>87.890857</td>\n",
       "      <td>95.296085</td>\n",
       "      <td>94.209742</td>\n",
       "      <td>96.272746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>west_texas</td>\n",
       "      <td>92.414727</td>\n",
       "      <td>96.012598</td>\n",
       "      <td>95.243173</td>\n",
       "      <td>96.861949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "0       sandy    80.626510            89.699062  86.721744       90.908122\n",
       "1     alberta    81.707195            92.493163  91.286594       93.649545\n",
       "2      boston    81.553379            91.001604  91.051350       92.469737\n",
       "3    oklahoma    84.367164            90.432033  90.962503       92.363534\n",
       "4  queensland    87.890857            95.296085  94.209742       96.272746\n",
       "5  west_texas    92.414727            96.012598  95.243173       96.861949"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W2V 500\n",
    "df_accs[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Naive Bayes</th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Neural Network</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sandy</th>\n",
       "      <td>79.138123</td>\n",
       "      <td>86.622049</td>\n",
       "      <td>85.962823</td>\n",
       "      <td>88.800225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alberta</th>\n",
       "      <td>83.272311</td>\n",
       "      <td>88.465729</td>\n",
       "      <td>88.844399</td>\n",
       "      <td>91.755516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boston</th>\n",
       "      <td>84.568941</td>\n",
       "      <td>89.872753</td>\n",
       "      <td>89.932853</td>\n",
       "      <td>91.910336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oklahoma</th>\n",
       "      <td>81.035067</td>\n",
       "      <td>83.336092</td>\n",
       "      <td>82.066053</td>\n",
       "      <td>87.659735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queensland</th>\n",
       "      <td>87.143453</td>\n",
       "      <td>92.575209</td>\n",
       "      <td>91.070620</td>\n",
       "      <td>95.206424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>west_texas</th>\n",
       "      <td>91.585256</td>\n",
       "      <td>93.414087</td>\n",
       "      <td>92.994222</td>\n",
       "      <td>95.752668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Naive Bayes  Logistic Regression        SVM  Neural Network\n",
       "sandy         79.138123            86.622049  85.962823       88.800225\n",
       "alberta       83.272311            88.465729  88.844399       91.755516\n",
       "boston        84.568941            89.872753  89.932853       91.910336\n",
       "oklahoma      81.035067            83.336092  82.066053       87.659735\n",
       "queensland    87.143453            92.575209  91.070620       95.206424\n",
       "west_texas    91.585256            93.414087  92.994222       95.752668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accs[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
